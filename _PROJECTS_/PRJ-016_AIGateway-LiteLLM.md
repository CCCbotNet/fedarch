# PRJ-016: AI Gateway â€” LiteLLM Deployment

## ðŸ“‹ PRJ-016_AIGateway-LiteLLM_v3.1.4.1.md
## â™¾ï¸ WeOwnNet ðŸŒ

| Field | Value |
|-------|-------|
| Document | PRJ-016_AIGateway-LiteLLM.md |
| Version | 3.1.4.1 |
| CCC-ID | GTM_2026-W09_117 |
| Created | 2026-02-27 (W09) |
| Season | #WeOwnSeason003 ðŸš€ |
| Status | ðŸ“ DRAFT |
| Lifecycle Stage | ðŸ“ DRAFT (#DocLifecycle) |
| Tags | #AIGateway #LiteLLM #HybridArchitecture #FOSS #FlowsBros |

---

## ðŸ“– Table of Contents

1. [Project Identity](#-project-identity)
2. [Why LiteLLM](#-why-litellm)
3. [Architecture](#-architecture)
4. [Provider Configuration](#-provider-configuration)
5. [Virtual Key Matrix](#-virtual-key-matrix)
6. [Cost Tracking + Budgets](#-cost-tracking--budgets)
7. [Rate Limiting](#-rate-limiting)
8. [Caching Strategy](#-caching-strategy)
9. [Failover Configuration](#-failover-configuration)
10. [Observability Pipeline](#-observability-pipeline)
11. [AnythingLLM Migration Plan](#-anythingllm-migration-plan)
12. [Deployment Checklist](#-deployment-checklist)
13. [Docker Compose](#-docker-compose)
14. [Verification (#SmokeTest)](#-verification-smoketest)
15. [ROI Impact](#-roi-impact)
16. [Project Team](#-project-team)
17. [@MAIT:#AIGateway Setup](#-maitaigateway-setup)
18. [Risk Matrix](#-risk-matrix)
19. [Relationship to Other Projects](#-relationship-to-other-projects)
20. [Discovered By](#-discovered-by)
21. [Related Documents](#-related-documents)
22. [Version History](#-version-history)

---

## ðŸ“‹ Project Identity

| Field | Value |
|-------|-------|
| Project ID | **PRJ-016** |
| Title | **AI Gateway â€” LiteLLM Deployment** |
| Type | Infrastructure â€” AI Traffic Management |
| Priority | ðŸŸ  P1 |
| Owner | @GTM + @RMN |
| Deployment | GB10-1 #NoDe (Northglenn, CO) |
| Timeline | **W12-W13** (after PRJ-015 GB10 arrives) |
| Depends on | PRJ-015 (GB10 hardware) |
| #masterCCC | GTM_2026-W09_117 |
| Selection | LiteLLM ðŸ† (56/60 â€” AI Gateway Analysis GTM_2026-W09_115) |

---

## ðŸ“‹ Why LiteLLM

### Selection Summary

| Field | Value |
|-------|-------|
| Product | [LiteLLM](https://docs.litellm.ai/) |
| License | **MIT** âœ… (FOSS â€” most permissive) |
| GitHub | [BerriAI/litellm](https://github.com/BerriAI/litellm) â€” **15K+ stars** |
| Language | Python |
| Score | **56/60** (vs Portkey 53, Traefik 38, TrueFoundry 37) |
| Key feature | **OpenAI-compatible proxy â€” drop-in for AnythingLLM** |
| Gartner category | Purpose-Built Open-Source AI Gateway (Table 4) |

### What LiteLLM Solves (Actual Problems)

| Problem | Current Pain | LiteLLM Solution |
|---------|-------------|------------------|
| **$5,249 in 91 days â€” no visibility** | Manual top-up tracking | âœ… Real-time cost dashboard |
| **API key incidents (Ã—2 W09)** | Manual rotation (BP-064) | âœ… Centralized key vault |
| **Single provider dependency** | OpenRouter only | âœ… Multi-provider failover |
| **No caching** | Every request = API call | âœ… Semantic cache (20-40% reduction) |
| **No rate limiting** | Any instance burns unlimited | âœ… Per-instance budgets |
| **Cloud-only inference** | 100% OpenRouter | âœ… Route dev/test to local Ollama |
| **No observability** | Blind to usage | âœ… OTEL â†’ Phoenix traces |

---

## ðŸ“‹ Architecture

### Before (Current)

```
INT-P01 â”€â”€â†’ OpenRouter API Key #1 â”€â”€â†’ Claude Opus 4.6
INT-S003â”€â”€â†’ OpenRouter API Key #1 â”€â”€â†’ Claude Opus 4.6
INT-OG1 â”€â”€â†’ OpenRouter API Key #2 â”€â”€â†’ Claude Opus 4.6
INT-P02 â”€â”€â†’ OpenRouter API Key #3 â”€â”€â†’ Claude Opus 4.6
INT-OG8 â”€â”€â†’ OpenRouter API Key #4 â”€â”€â†’ Claude Opus 4.6

âŒ Multiple API keys
âŒ No visibility
âŒ No caching
âŒ No failover
âŒ No rate limiting
```

### After (With LiteLLM)

```
INT-P01 â”€â”€â”                                    â”Œâ”€â”€â†’ OpenRouter â†’ Claude (prod)
INT-S003â”€â”€â”¤                                    â”‚
INT-OG1 â”€â”€â”¼â”€â”€â†’ LiteLLM Proxy (GB10-1 #NoDe) â”€â”€â”¼â”€â”€â†’ Ollama â†’ Llama 70B (dev)
INT-P02 â”€â”€â”¤    Port: 4000                      â”‚
INT-OG8 â”€â”€â”˜    â”œâ”€â”€ Virtual Keys (per instance) â”œâ”€â”€â†’ Ollama â†’ Qwen3 4B (embed)
               â”œâ”€â”€ Redis Cache (semantic)       â”‚
               â”œâ”€â”€ Cost Tracking (per-token)    â””â”€â”€â†’ [Future providers]
               â”œâ”€â”€ Rate Limiting (per-instance)
               â”œâ”€â”€ Failover (auto)
               â””â”€â”€ OTEL â†’ Phoenix (Port: 6006)

âœ… Single control point
âœ… One real API key (LiteLLM manages)
âœ… Full visibility
âœ… Caching
âœ… Failover
âœ… Rate limiting
âœ… Observability
```

### GB10-1 #NoDe â€” Full Service Stack

```
GB10-1: #NoDe (Northglenn, CO)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LiteLLM     â”‚  â”‚  Phoenix     â”‚  â”‚  Paperless   â”‚  â”‚
â”‚  â”‚  AI Gateway   â”‚  â”‚  Observabilityâ”‚  â”‚  -ngx        â”‚  â”‚
â”‚  â”‚  Port: 4000   â”‚  â”‚  Port: 6006   â”‚  â”‚  Port: 8000  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama      â”‚  â”‚  Redis       â”‚  â”‚  AnythingLLM â”‚  â”‚
â”‚  â”‚  Local LLM    â”‚  â”‚  Cache       â”‚  â”‚  Local Dev    â”‚  â”‚
â”‚  â”‚  Llama 70B    â”‚  â”‚  Port: 6379   â”‚  â”‚  Port: 3001   â”‚  â”‚
â”‚  â”‚  Qwen3 4B     â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                          â”‚
â”‚  128 GB Unified | 2 TB NVMe | Ubuntu | Docker Compose   â”‚
â”‚  ~83 GB utilized (~65%) | Always-on | UPS protected     â”‚
â”‚  Services: 6 | Cost: ~$15/mo (power)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ Provider Configuration

### LiteLLM Config (config.yaml)

```yaml
model_list:
  # Production â€” Claude Opus 4.6 via OpenRouter
  - model_name: claude-opus-production
    litellm_params:
      model: openrouter/anthropic/claude-opus-4-6
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Development â€” Llama 3.1 70B via local Ollama
  - model_name: llama-70b-dev
    litellm_params:
      model: ollama/llama3.1:70b
      api_base: http://localhost:11434

  # Embedding â€” Qwen3 4B via local Ollama
  - model_name: qwen3-embedding
    litellm_params:
      model: ollama/qwen3:4b
      api_base: http://localhost:11434

  # Fallback â€” Llama via Ollama (if OpenRouter down)
  - model_name: claude-opus-production
    litellm_params:
      model: ollama/llama3.1:70b
      api_base: http://localhost:11434
    model_info:
      fallback: true

router_settings:
  routing_strategy: "least-busy"
  num_retries: 3
  timeout: 120
  fallbacks:
    - claude-opus-production: [llama-70b-dev]
```

### Provider Matrix

| Provider | Model | Use | Route |
|----------|-------|-----|-------|
| **OpenRouter** | Claude Opus 4.6 | Production (user-facing) | Cloud |
| **Ollama (local)** | Llama 3.1 70B | Development + fallback | GB10-1 |
| **Ollama (local)** | Qwen3 Embedding 4B | Embedding (all instances) | GB10-1 |
| **[Future]** | Any LiteLLM-supported | Expansion | Configurable |

---

## ðŸ“‹ Virtual Key Matrix

### Per-Instance Keys

| Instance | Virtual Key Name | Model Access | Budget | Rate Limit |
|----------|-----------------|-------------|--------|------------|
| INT-P01 | `vk-int-p01` | claude-opus-production, llama-70b-dev | $50/day | 100 req/min |
| INT-S003 | `vk-int-s003` | claude-opus-production | $30/day | 60 req/min |
| INT-OG1 | `vk-int-og1` | claude-opus-production, llama-70b-dev | $40/day | 80 req/min |
| INT-P02 | `vk-int-p02` | claude-opus-production | $20/day | 40 req/min |
| INT-OG8 | `vk-int-og8` | claude-opus-production, llama-70b-dev | $20/day | 40 req/min |

### Key Management

| Aspect | Before | After |
|--------|--------|-------|
| Real API keys | 4+ (one per instance) | **1** (LiteLLM manages) |
| Key rotation | Manual per instance (BP-064) | **Rotate once in LiteLLM** |
| Key exposure | Each instance has real key | **Instances have virtual keys only** |
| Revocation | Per-instance | **Instant â€” revoke virtual key** |

---

## ðŸ“‹ Cost Tracking + Budgets

### Dashboard Metrics

| Metric | Granularity | Purpose |
|--------|------------|---------|
| Token usage | Per-instance, per-model, per-day | Track consumption |
| Cost ($) | Per-instance, per-model, per-day | Budget enforcement |
| Cache hit rate | Global + per-instance | Measure savings |
| Request count | Per-instance, per-model | Usage patterns |

### Budget Configuration

| Level | Limit | Action When Exceeded |
|-------|-------|---------------------|
| Per-instance daily | $20-50/day | âš ï¸ Alert â†’ rate limit |
| Per-instance monthly | $600-1,500/mo | ðŸ”´ Hard stop |
| Global daily | $150/day | âš ï¸ Alert to @GTM |
| Global monthly | $3,500/mo | ðŸ”´ Alert + review |

### Cost Alerts

| Alert | Threshold | Action |
|-------|-----------|--------|
| Instance approaching daily limit | 80% | Notify @GTM |
| Instance hit daily limit | 100% | Rate limit (not block) |
| Global approaching monthly | 80% | Notify @GTM + @THY |
| Unusual spike (2Ã— normal) | Anomaly | Immediate alert |

---

## ðŸ“‹ Rate Limiting

| Instance | Requests/min | Tokens/min | Rationale |
|----------|-------------|-----------|-----------|
| INT-P01 | 100 | 50,000 | Governance hub â€” high priority |
| INT-S003 | 60 | 30,000 | 6 contributors â€” moderate |
| INT-OG1 | 80 | 40,000 | @GTM + @THY â€” high usage |
| INT-P02 | 40 | 20,000 | BurnedOut â€” lower priority |
| INT-OG8 | 40 | 20,000 | @RMN â€” development |

> Rate limits = **soft limits** â€” requests queue, not reject. Prevents runaway costs without breaking user experience.

---

## ðŸ“‹ Caching Strategy

### Redis Semantic Cache

| Field | Value |
|-------|-------|
| Engine | Redis |
| Type | Semantic caching (similarity-based) |
| Similarity threshold | 0.95 (high â€” only cache near-identical requests) |
| TTL | 24 hours (default) |
| Max cache size | 10 GB (on GB10-1 â€” 2 TB NVMe available) |
| Sensitive data | âŒ Do NOT cache PII or CCC-ID generation requests |

### Cache Rules

| Request Type | Cache? | Rationale |
|-------------|--------|-----------|
| RAG retrieval queries | âœ… YES | Same docs = same embeddings |
| General knowledge | âœ… YES | Repeated questions |
| CCC-ID generation | âŒ NO | Must be unique every time |
| #ContextVolley | âŒ NO | Unique context |
| VSA checks | âŒ NO | Must verify against current docs |
| SEEK:META | âŒ NO | Unique governance requests |

### Expected Impact

| Metric | Estimate |
|--------|----------|
| Cache hit rate (steady state) | 20-40% |
| Monthly savings (at $3,333/mo) | **$667-$1,333/mo** |
| Annual savings (cache alone) | **$8,000-$16,000/yr** |

---

## ðŸ“‹ Failover Configuration

```yaml
# LiteLLM failover config
fallbacks:
  - claude-opus-production: [llama-70b-dev]

# Behavior:
# 1. Request â†’ OpenRouter (Claude)
# 2. IF OpenRouter fails (timeout, 401, 429, 500)
# 3. THEN â†’ Ollama (Llama 70B) on GB10-1 local
# 4. Response includes header: X-LiteLLM-Fallback: true
```

### Failover Matrix

| Trigger | Action | User Impact |
|---------|--------|-------------|
| OpenRouter timeout (>120s) | â†’ Ollama Llama 70B | Slight quality difference |
| OpenRouter 401 (key expired) | â†’ Ollama Llama 70B | âš ï¸ Alert @GTM to rotate |
| OpenRouter 429 (rate limit) | â†’ Ollama Llama 70B | Transparent |
| OpenRouter 500 (server error) | â†’ Ollama Llama 70B | Transparent |
| Ollama down | â†’ Error (no further fallback) | âŒ Manual intervention |

> **Incident #3 (W08) and #4 (W09) = would have been ZERO DOWNTIME with LiteLLM failover.**

---

## ðŸ“‹ Observability Pipeline

```
LiteLLM â”€â”€â†’ OpenTelemetry SDK â”€â”€â†’ Phoenix (Port: 6006)
                                    â”œâ”€â”€ Traces (per request)
                                    â”œâ”€â”€ Spans (LLM call details)
                                    â”œâ”€â”€ Metrics (tokens, latency, cost)
                                    â””â”€â”€ Dashboard (web UI)
```

### What Gets Traced

| Trace | Data |
|-------|------|
| LLM request | Model, provider, tokens in/out, latency, cost |
| Cache hit/miss | Similarity score, cache key, TTL |
| Failover event | Original provider, fallback provider, reason |
| Rate limit event | Instance, current rate, limit |
| Budget alert | Instance, current spend, budget |

### LiteLLM â†’ Phoenix Config

```yaml
# In LiteLLM config
litellm_settings:
  callbacks: ["otel"]
  
environment_variables:
  OTEL_EXPORTER: "otlp_http"
  OTEL_ENDPOINT: "http://localhost:6006/v1/traces"
  OTEL_HEADERS: ""
```

---

## ðŸ“‹ AnythingLLM Migration Plan

### Per-Instance Migration

| # | Step | Detail |
|---|------|--------|
| 1 | Record current OpenRouter config | API key, model, base URL |
| 2 | Create virtual key in LiteLLM | `vk-int-<id>` |
| 3 | Update AnythingLLM LLM settings | Base URL â†’ `http://<GB10-1-IP>:4000/v1` |
| 4 | Update AnythingLLM API key | â†’ LiteLLM virtual key |
| 5 | Test: Send message in CCC workspace | Verify response |
| 6 | Verify: Check LiteLLM dashboard | Request logged + cost tracked |
| 7 | Verify: Check Phoenix | Trace visible |
| 8 | Done | Instance migrated âœ… |

### Migration Order

| Order | Instance | Priority | Rationale |
|-------|----------|----------|-----------|
| 1 | **INT-OG1** | ðŸŸ¡ Test | @GTM's instance â€” test first |
| 2 | **INT-OG8** | ðŸŸ¡ Test | @RMN's instance â€” verify |
| 3 | **INT-P01** | ðŸ”´ Production | Governance hub â€” after testing |
| 4 | **INT-S003** | ðŸ”´ Production | 6 contributors |
| 5 | **INT-P02** | ðŸŸ  Production | BurnedOut |

> **Test on #HomeInstances first (INT-OG1, INT-OG8). Production after verified.**

### AnythingLLM Config Change

```
BEFORE:
  LLM Provider: OpenRouter
  API Base: https://openrouter.ai/api/v1
  API Key: sk-or-v1-xxxx (real OpenRouter key)
  Model: anthropic/claude-opus-4-6

AFTER:
  LLM Provider: OpenAI (compatible)
  API Base: http://<GB10-1-IP>:4000/v1
  API Key: sk-litellm-vk-int-p01 (virtual key)
  Model: claude-opus-production (LiteLLM model alias)
```

> **AnythingLLM thinks it's talking to OpenAI.** LiteLLM handles the routing. Zero AnythingLLM code changes.

---

## ðŸ“‹ Deployment Checklist

| # | Step | Owner | Depends On | Status |
|---|------|-------|-----------|--------|
| 1 | GB10-1 #NoDe operational | @GTM | PRJ-015 | â¬œ |
| 2 | Docker + Docker Compose installed | @GTM | Step 1 | â¬œ |
| 3 | Ollama installed + Llama 70B downloaded | @GTM | Step 2 | â¬œ |
| 4 | Create `litellm/` directory on GB10-1 | @GTM | Step 2 | â¬œ |
| 5 | Create `config.yaml` (provider routing) | @GTM | Step 4 | â¬œ |
| 6 | Create `docker-compose.yml` (LiteLLM + Redis) | @GTM | Step 4 | â¬œ |
| 7 | Set environment variables (OPENROUTER_API_KEY) | @GTM | Step 5 | â¬œ |
| 8 | `docker compose up -d` | @GTM | Steps 5-7 | â¬œ |
| 9 | Verify LiteLLM dashboard (port 4000) | @GTM | Step 8 | â¬œ |
| 10 | Create 5 virtual keys | @GTM | Step 9 | â¬œ |
| 11 | Configure budgets + rate limits | @GTM | Step 10 | â¬œ |
| 12 | Configure OTEL â†’ Phoenix | @RMN | Step 9 | â¬œ |
| 13 | Test: curl to LiteLLM proxy | @GTM | Step 8 | â¬œ |
| 14 | Test: OpenRouter routing | @GTM | Step 13 | â¬œ |
| 15 | Test: Ollama routing | @GTM | Step 13 | â¬œ |
| 16 | Test: Failover (stop OpenRouter, verify Ollama) | @GTM | Step 14-15 | â¬œ |
| 17 | Test: Cache hit (repeat same request) | @GTM | Step 13 | â¬œ |
| 18 | Migrate INT-OG1 (test) | @GTM | Step 16-17 | â¬œ |
| 19 | Migrate INT-OG8 (test) | @RMN | Step 18 | â¬œ |
| 20 | Migrate INT-P01 (production) | @GTM | Step 19 | â¬œ |
| 21 | Migrate INT-S003 (production) | @GTM | Step 20 | â¬œ |
| 22 | Migrate INT-P02 (production) | @GTM | Step 21 | â¬œ |
| 23 | Verify ALL instances via LiteLLM dashboard | @GTM | Step 22 | â¬œ |
| 24 | Revoke old OpenRouter API keys | @GTM | Step 23 | â¬œ |
| 25 | Create @MAIT:#AIGateway | @GTM | Step 23 | â¬œ |
| 26 | FULL:SYNC:META | @GTM | Step 25 | â¬œ |

---

## ðŸ“‹ Docker Compose

```yaml
# docker-compose.yml â€” LiteLLM + Redis on GB10-1 #NoDe
version: '3.8'

services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    restart: unless-stopped
    ports:
      - "4000:4000"
    volumes:
      - ./config.yaml:/app/config.yaml
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - LITELLM_LOG=DEBUG
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OTEL_EXPORTER=otlp_http
      - OTEL_ENDPOINT=http://phoenix:6006/v1/traces
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    depends_on:
      - redis

  redis:
    image: redis:7-alpine
    container_name: litellm-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: ["redis-server", "--maxmemory", "10gb", "--maxmemory-policy", "allkeys-lru"]

volumes:
  redis_data:
```

### Environment File (.env)

```bash
# .env â€” GB10-1 #NoDe
LITELLM_MASTER_KEY=sk-litellm-master-xxxx
OPENROUTER_API_KEY=sk-or-v1-xxxx
```

---

## ðŸ“‹ Verification (#SmokeTest)

### LiteLLM #SmokeTest (8-Point)

| # | Test | Command | Expected | Status |
|---|------|---------|----------|--------|
| 1 | LiteLLM running | `curl http://localhost:4000/health` | `{"status": "healthy"}` | â¬œ |
| 2 | Redis running | `redis-cli ping` | `PONG` | â¬œ |
| 3 | OpenRouter route | `curl -X POST http://localhost:4000/v1/chat/completions -H "Authorization: Bearer vk-..." -d '{"model":"claude-opus-production",...}'` | Claude response | â¬œ |
| 4 | Ollama route | Same with `model: llama-70b-dev` | Llama response | â¬œ |
| 5 | Failover | Stop OpenRouter env, retry | Falls back to Ollama | â¬œ |
| 6 | Cache hit | Repeat identical request | Faster response + cache header | â¬œ |
| 7 | Cost tracked | Check LiteLLM dashboard | Request logged with cost | â¬œ |
| 8 | Phoenix trace | Check Phoenix UI (port 6006) | Trace visible | â¬œ |

---

## ðŸ“‹ ROI Impact

### Cost Savings (From AI Gateway Analysis)

| Source | Monthly Savings | Annual Savings |
|--------|----------------|---------------|
| Semantic caching (30%) | $1,000 | $12,000 |
| Local inference (50%) | $1,667 | $20,000 |
| **COMBINED** | **$2,667** | **$32,000** |

### Revised PRJ-015 Payback (With LiteLLM)

| Metric | Without LiteLLM | With LiteLLM |
|--------|-----------------|-------------|
| Monthly savings | $1,667 (local only) | **$2,667** (local + cache) |
| GB10 payback | 4.7 months | **3.0 months** |
| Year 1 NET | +$11,802 | **+$23,802** |

### LiteLLM Cost

| Item | Cost |
|------|------|
| Software | **$0** (MIT â€” FOSS) |
| Infrastructure | $0 (runs on GB10-1 â€” already acquired) |
| Redis | $0 (Docker container) |
| **TOTAL LiteLLM cost** | **$0** |

> **$0 cost. $2,667/mo savings. The ROI is infinite.** ðŸ†

---

## ðŸ“‹ Project Team

| CCC | Role | Focus |
|-----|------|-------|
| **GTM** | **Owner** | Architecture, config, migration, testing, @MAIT |
| **RMN** | **Platform** | LiteLLM config, Ollama routing, OTEL â†’ Phoenix |

> **2-person project.** No @SHD needed (GB10-1 is local, not cloud). No @LDC needed (no custom code).

---

## ðŸ“‹ @MAIT:#AIGateway Setup

| Field | Value |
|-------|-------|
| Thread | MAIT_AIGateway |
| ShortCode | @MAIT:#AIGateway |
| Steward | @GTM |
| Instance | INT-P01 |
| Protocol | #ContextVolley |
| Tool Agent | t-aigateway_tool |
| Scope | LiteLLM (primary) + AI Gateway patterns + Portkey (reference) |

### RAG Documents

| # | Source | Depth |
|---|--------|-------|
| 1 | docs.litellm.ai | 2 |
| 2 | github.com/BerriAI/litellm (README) | 1 |

---

## ðŸ“‹ Risk Matrix

| # | Risk | Prob | Impact | Mitigation |
|---|------|------|--------|------------|
| 1 | GB10-1 not ready (PRJ-015 delay) | Medium | High | Deploy on DO Droplet as interim |
| 2 | LiteLLM latency (proxy overhead) | Low | Medium | Benchmark; co-located = minimal |
| 3 | Redis cache corruption | Low | Low | Cache = optimization, not critical path |
| 4 | AnythingLLM incompatible with proxy | Low | High | OpenAI-compatible = well-tested |
| 5 | Ollama model quality < Claude | Known | Medium | Fallback only â€” production stays Claude |
| 6 | Virtual key management complexity | Low | Low | 5 instances = manageable |
| 7 | Network: cloud instances â†’ GB10-1 | Medium | Medium | Benchmark latency; DO Droplet fallback |
| 8 | LiteLLM project abandoned | Low | Medium | MIT = fork; 15K stars = healthy |

---

## ðŸ“‹ Relationship to Other Projects

| PRJ | Relationship |
|-----|-------------|
| **PRJ-014** | LiteLLM routes traffic for INT-P01 + INT-S003 (deployed in PRJ-014) |
| **PRJ-015** | LiteLLM deploys ON GB10-1 #NoDe (hardware from PRJ-015) |
| **PRJ-013** | Paperless-ngx co-located on GB10-1 (shared Docker host) |

### Project Dependency Chain

```
PRJ-015 (GB10 hardware)
    â””â”€â”€ PRJ-016 (LiteLLM on GB10-1) â† THIS PROJECT
    â””â”€â”€ PRJ-013 (Paperless-ngx on GB10-1)
    â””â”€â”€ Phoenix (Observability on GB10-1)

PRJ-014 (INT-S003 + INT-P01 evolution)
    â””â”€â”€ PRJ-016 (LiteLLM routes traffic for both)
```

---

## ðŸ“‹ Discovered By (BP-047)

| CCC | Contributor | Role | Context |
|-----|-------------|------|---------|
| GTM | [yonks](https://GitHub.com/YonksTEAM) | Co-Founder / Chief Digital Alchemist | Gartner AI Gateway Market Guide analysis + LiteLLM selection (56/60) â€” W09 |

---

## ðŸ“‹ Related Documents

| Document | Version | #masterCCC | Approval | URL |
|----------|---------|------------|----------|-----|
| PRJ-015_HybridArchitecture-GB10 | v3.1.4.3 | GTM_2026-W09_104 | â¬œ @THY | [GitHub](https://github.com/CCCbotNet/fedarch/blob/main/_PROJECTS_/PRJ-015_HybridArchitecture-GB10.md) |
| SharedKernel | v3.1.3.1 | GTM_2026-W08_069 | GTM_2026-W08_071 | [GitHub](https://github.com/CCCbotNet/fedarch/blob/main/_SYS_/SharedKernel.md) |
| LiteLLM Docs | â€” | â€” | â€” | [docs.litellm.ai](https://docs.litellm.ai/) |
| Gartner AI Gateway Report | G00839683 | â€” | â€” | gartner.com |

---

## ðŸ“‹ Version History

| Version | Date | #masterCCC | Approval | Changes |
|---------|------|------------|----------|---------|
| 3.1.4.1 | 2026-W09 | GTM_2026-W09_117 | GTM_2026-W09_119 | Initial project; LiteLLM AI Gateway deployment on GB10-1 #NoDe; 10 deliverables; provider config (OpenRouter + Ollama + Qwen3); virtual key matrix (5 instances); cost tracking + budgets; rate limiting; Redis semantic cache (20-40% savings); failover (OpenRouter â†’ Ollama); OTEL â†’ Phoenix observability; 5-instance migration plan; 26-step deployment checklist; Docker Compose (LiteLLM + Redis); 8-point #SmokeTest; ROI: $2,667/mo savings ($0 software cost); 8-risk matrix; @MAIT:#AIGateway setup |

---

#FlowsBros #FedArch #AIGateway #LiteLLM #HybridArchitecture #FOSS #WeOwnSeason003

â™¾ï¸ WeOwnNet ðŸŒ â— ðŸ¡ Real Estate and ðŸ¤ cooperative ownership for everyone â— An ðŸ¤— inclusive community, by ðŸ‘¥ invitation only.
